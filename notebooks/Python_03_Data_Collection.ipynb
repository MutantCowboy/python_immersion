{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Immersion Course\n",
    "\n",
    "### Data Collection with Python - Part 3\n",
    "\n",
    "### Joe Blankenship - Just some dude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have a good grasp of Python's basic fucntionality, you can interact with a number of data sources. This section will focus on the basics of extracting, tranforming, and loading data formats into dataframes for analysis. Data manipulation inside of the dataframes will be saved for Part 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Web Scraping\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first scraper we'll build will use core Python libraries to:\n",
    "\n",
    "* Go to a HTTP website\n",
    "* Gather the source code\n",
    "* Print the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we'll import urllib, io, and pprint modules to obtain out data\n",
    "\n",
    "from urllib.request import Request, urlopen\n",
    "from io import TextIOWrapper\n",
    "from pprint import pprint\n",
    "\n",
    "# Declare the URL\n",
    "url = 'https://en.wikipedia.org/wiki/Department_of_Geography,_University_of_Kentucky'\n",
    "\n",
    "# Open the URL\n",
    "page = Request(url)\n",
    "page_content = urlopen(page)\n",
    "# page_content.read()\n",
    "\n",
    "# Buffer our text stream from the website\n",
    "page_data = TextIOWrapper(page_content)\n",
    "\n",
    "# pprint out our data\n",
    "for row in page_data:\n",
    "    pprint(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we may want something a bit more elegant. This is where `requests` and `beautifulsoup` comes in to help us out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import requests and beautifulsoup\n",
    "# Import pandas, we'll use that at the end\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# we are going to scrape crime data from the UK crime http://www.uky.edu/crimelog/\n",
    "# substitute variables to fill in REST query criteria\n",
    "start_month, start_day, start_year = 1, 1, 2018\n",
    "end_month, end_day, end_year = 10, 4, 2018\n",
    "crime_data_raw = requests.get('http://www.uky.edu/crimelog/log?field_log_category_value=All' +\n",
    "                              '&field_log_report_value%5Bmin%5D%5Bmonth%5D=' + str(start_month) +\n",
    "                              '&field_log_report_value%5Bmin%5D%5Bday%5D=' + str(start_day) +\n",
    "                              '&field_log_report_value%5Bmin%5D%5Byear%5D=' + str(start_year) +\n",
    "                              '&field_log_report_value%5Bmax%5D%5Bmonth%5D=' + str(end_month) +\n",
    "                              '&field_log_report_value%5Bmax%5D%5Bday%5D=' + str(end_day) +\n",
    "                              '&field_log_report_value%5Bmax%5D%5Byear%5D=' + str(end_year)\n",
    "                             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a soup object \n",
    "crime_bs_proc = BeautifulSoup((crime_data_raw.text), \"html5lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a filter for our soup object to pull out the table\n",
    "crime_data_table = crime_bs_proc.find('table', {'class': 'views-table cols-8'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the table header in the data\n",
    "crime_data_header = crime_data_table.find('thead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all the table headers\n",
    "crime_data_heads = crime_data_header.find_all('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty list for the header\n",
    "header = []\n",
    "\n",
    "# iterate through the header element to get text\n",
    "for col in crime_data_heads:\n",
    "    cols = col.find_all('a')\n",
    "    cols = [ele.text.strip() for ele in cols]\n",
    "    header.append([ele for ele in cols if ele])\n",
    "\n",
    "# flatten the list to a single list\n",
    "header = [item for sublist in header for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the table rows in the data\n",
    "crime_data_body = crime_data_table.find('tbody')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all table rows\n",
    "crime_data_rows = crime_data_body.find_all('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty list for the rows of data\n",
    "data = []\n",
    "\n",
    "# iterate through the header element to get the rows\n",
    "for row in crime_data_rows:\n",
    "    cols = row.find_all('td')\n",
    "    cols = [ele.text.strip() for ele in cols]\n",
    "    data.append([ele for ele in cols if ele])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with our data using our header list\n",
    "uk_crime_data = pd.DataFrame(data, columns=header)\n",
    "uk_crime_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also the `scrapy` library in Python for more intense scraping projects with more considerations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## APIs\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "APIs often have 'wrappers' in Python that you can use to interface with the underlying data.\n",
    "\n",
    "Here we will use the data.world API to import some data\n",
    "\n",
    "  * docs at https://github.com/datadotworld/data.world-py\n",
    "\n",
    "Prior to this, you should load your API credentials from data.world into your active virtual env (in the terminal)\n",
    "\n",
    "`export DW_AUTH_TOKEN=<YOUR_TOKEN>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our API library\n",
    "\n",
    "import datadotworld as dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our data sets from the API using a known user data collection\n",
    "\n",
    "lex_business_health = dw.load_dataset('inform8n/most-recent-lexington-ky-health-department-inspection-scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the dataframes available in the data set collection\n",
    "\n",
    "lex_business_health.dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a data set into a dataframe from the data collection\n",
    "\n",
    "food_scores = lex_business_health.dataframes.get('most_recent_food_scores')\n",
    "food_scores.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Flat files\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Streaming\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<hr>\n",
    "\n",
    "## Resources\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Note:** A lot of the open-source materials are provided by people who develop those materials for a living. So please consider sending them a thank you and if you can, a few buck to support their efforts. Thanks! :)    \n",
    "\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
